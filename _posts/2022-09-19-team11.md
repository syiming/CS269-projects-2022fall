---
layout: post
comments: true
title: Moving State-of-the-art Models in MetaDrive
author: Siqi Liu, Yiming Shi (Team 11)
date: 2022-10-12
---

Model 1: EPGO
Model 2: Implicit Affordance

> Many State-of-the-art results for reinforcement learning in autonomous driving are demonstrated over CARLA. We aim to reproduce these results in Metadrives, and compare them with metadrive models. 
<!-- (prev proposal)> Ususally driving involves multiple intelligent agents, intelligent machine or human. With more information cooperated, agents may performs differently to achieve their goals. In this projects, we want to research about Reinforcement Learning (RL) based method such that all the agents collaborate with each other to achieve the best goal in total. -->
<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction 
Recently, Reinforcement learning has been wildly used in autonomous driving. Since the driving task has high complexity and hard to construct a safe model or policy, imitation learning by behavior cloning becomes popular as it allows the agent to be trained to learn from a human expert and thus quickly learn to plan the action based on the complex environment and take actions. However, imitation learning is also suffered from issues such as failing unseen complex scenarios. In this project, we are aiming on reproducing a safe driving model based on imitation learning, analyzing failing cases for imitation learning and exploring ways such as adding loss besides imitation loss to discourage certain undesired behavior.

<!-- As 5G techniques become more mature, vehicle-to-everything (V2X) becomes possible. This allows vehicles to communicate with any entity that may affect, or may be affected by, the vehicle. A vehicle may use this information to act differently to achieve the final goal.  -->


## Proposed Work
Our project should be mainly two parts. In the first part, we will reproduce the result using imitation learning. In the second phase, we will try to add a policy or policy-based learning to encourage the desired behavior and punish the undesired behavior. We will test these under restrictions such as stop signs, speed limitations, and so on.

## Reproducing state-of-the-art result in Metadrive. 

We have tested the code of EGPO on metadrive. We ran the baseline models of EGPO, including PPO and SAC model. 
The result of the baseline models seems to be consistent with the paper; however, due to the resource constraint, we are not able to run all the seeds. 
Further, seems to see error in running the EGPO's original training code. 
We are also testing tuning of the SAC and PPO model, including using priorized replay. 

## Expected Result
We will first try to reproduce the result with imitation learning, we are expecting the agent are able to dring under simple environment without crashing. We alse expect that the result with imitation learning plus other reinforcement learning, we will reach much better result on complex senarios. 
Concretely, we will allow our agent to learn from the human driving example, then use reinforcement learning to broaden the model's training to create a safer model. 

We will first reproduce the imitation learning in CheuffeurNet. Then, we will use a reinforcement technique similar to Deep q-learning from demonstrations. 
In the process, we may face obstacles in combining the model's approaches and training curves, and a solution may be to perform our training by stages, by first using CheuffeurNet's training, then DqFD's. 
Therefore, we expect to deliver a method of training autonomous driving with a combination of imitation learning and reinforcement learning to the readers. 

## Related Work

Our Imitation learning work will follow CheuffeurNet[1]. 
Multiple similar works also use Imitation learning[2, 4].
Then, we perform the combination of Imitation learning and reinforcement learning following Deep d-learning from the demonstration[5].

### Platform
The platform we are going to work on is Metadrive[6]. It is an open-source driving simulator that is able to construct simple and complex scenarios.

## References
1. Mayank Bansal, Alex Krizhevsky, & AbhÄ³it S. Ogale (2018). ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst. CoRR, abs/1812.03079.
2. Haan, P.D., Jayaraman, D., & Levine, S. (2019). Causal Confusion in Imitation Learning. NeurIPS.
3. Jinyun Zhou, Rui Wang, Xu Liu, Yifei Jiang, Shu Jiang, Jiaming Tao, Jinghao Miao, & Shiyu Song (2021). Exploring Imitation Learning for Autonomous Driving with Feedback Synthesizer and Differentiable Rasterization. CoRR, abs/2103.01882.
4. Hester, Todd, et al. "Deep q-learning from demonstrations." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.
5. Peng, Z, Li, Q & Liu, C & Zhou, B. "Safe Driving via Expert Guided Policy Optimization." 5th Annual Conference on Robot Learning. 2021.
6. Li, Quanyi and Peng, Zhenghao and Xue, Zhenghai and Zhang, Qihang and Zhou, Bolei. "MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning." arXiv. 2021

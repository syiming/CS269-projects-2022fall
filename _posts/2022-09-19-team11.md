---
layout: post
comments: true
title: Move and Combine State-of-the-art Models in MetaDrive
author: Siqi Liu, Yiming Shi (Team 11)
date: 2022-10-12
---

> Many State-of-the-art results for reinforcement learning in autonomous driving are demonstrated over CARLA, VISTA. We aim to reproduce these results in Metadrive simulator, and compare them with Metadrive models. We will also investigate the combinations of different models in prediction stage.
<!-- (prev proposal)> Ususally driving involves multiple intelligent agents, intelligent machine or human. With more information cooperated, agents may performs differently to achieve their goals. In this projects, we want to research about Reinforcement Learning (RL) based method such that all the agents collaborate with each other to achieve the best goal in total. -->
<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction 
<!--Recently, Reinforcement learning has been wildly used in autonomous driving. Since the driving task has high complexity and hard to construct a safe model or policy, imitation learning by behavior cloning becomes popular as it allows the agent to be trained to learn from a human expert and thus quickly learn to plan the action based on the complex environment and take actions. However, imitation learning is also suffered from issues such as failing unseen complex scenarios. In this project, we are aiming on reproducing a safe driving model based on imitation learning, analyzing failing cases for imitation learning and exploring ways such as adding loss besides imitation loss to discourage certain undesired behavior.-->

<!-- As 5G techniques become more mature, vehicle-to-everything (V2X) becomes possible. This allows vehicles to communicate with any entity that may affect, or may be affected by, the vehicle. A vehicle may use this information to act differently to achieve the final goal.  -->

Recently, Reinforcement learning has been wildly used in autonomous driving and the driving task has high complexity and hard to construct a safe model or policy. Nowadays many different methods are developed and used in reinforcement learning, such as Deep Q-Network (Doubling DQN, Dueling DQN, Rainbow DQN), Policy Gradient, Natrual Policy Gradient, Proximal Policy Optimization, Advantage Actor-Critic, to name a few. A lot of these models are demonstrated on CARLA, which is a very popular simulator in autonomous driving. In this project, we are aiming at applying different models under a much light weighted and easy used simulator - Metadrive[1] and compare the result of these models.


## Proposed Work
The main goal of this project is to apply the following models using Metadrive and compare the result of these models. We also trying to reproduce some state-of-art models on Metadrive.
1. DQN
2. Double DQN
3. Rainbow DQN
4. Quantile Regressing DQN
5. Policy Gradient
6. Proximsl Policy Optimization
7. Deep Deterministic Policy Gradient
8. Soft Actor-Critic
9. EGPO

More models are going to be added.

In this project, we are also going compare different models about their advantange or disadvantage of each models (training time, performance, etc). We will also construct different senarios for different model to solve, we will compare the success rate, training time on these models under different senarios.

We also want to propose a way to combine different models in prediction stage. (eg. Majority vote for next action. If the action contains a continuous value, we could do an weighted sum.)

##  Reproducing SOTA Models in Metadrive

During this time, we spend some time investigating and understading different models, such as DQN, Double DQN, Rainbow DQN, Quantile Regressing DQN, Policy Gradient, Proximsl Policy Optimization, Deep Deterministic Policy Gradient, Soft Actor-Critic, EGPO.

Currently, we have installed and test several simple models on Metadrive, eg. DQN, Double DQN, PPO, Soft Actor-Critic, and EGPO.
The result of the models seems to be consistent with the paper; however, due to the resource constraint, we are not able to run all the seeds.

We are also testing tuning of the Soft Actor-Critic and PPO model, including using priorized replay. 

![ppo]({{ '/assets/images/team11/ppo.jpeg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
![sac]({{ '/assets/images/team11/sac.jpeg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}


Currently, we are continuing trying to work on more models with metadrive, and hopefully we can reproduce and visualize the result soon.


## Expected Result
We expect different result using different models with different training time, but for a combined policy with multiple models combined, we expect it to have better performance. We will also compare the perfomance for different models under simple and complex senarios separately. We anticipate the agent will reach a good performance on simple senario very quickly and for different models the time to reach the state will be different.

We will also train this models under different senarios, we expect the models are tend to solve the easy senarios very quick and might not solve the complex senarios due to computation resource limitation. The training time will be different and the action might be different by different models. We will try to analyze the differences for different models.

<!-- We will first try to reproduce the result with imitation learning, we are expecting the agent are able to dring under simple environment without crashing. We alse expect that the result with imitation learning plus other reinforcement learning, we will reach much better result on complex senarios. 
Concretely, we will allow our agent to learn from the human driving example, then use reinforcement learning to broaden the model's training to create a safer model. 

We will first reproduce the imitation learning in CheuffeurNet. Then, we will use a reinforcement technique similar to Deep q-learning from demonstrations. 
In the process, we may face obstacles in combining the model's approaches and training curves, and a solution may be to perform our training by stages, by first using CheuffeurNet's training, then DqFD's. 
Therefore, we expect to deliver a method of training autonomous driving with a combination of imitation learning and reinforcement learning to the readers.  -->

## Related Work

Currently, many reinforcement learning models are proposed and achieve great performance. We investiage many of these DQN[2], Double DQN[3], Rainbow DQN[4], Quantile Regressing DQN[5], Policy Gradient[6], Proximsl Policy Optimization[7], Deep Deterministic Policy Gradient[8], Soft Actor-Critic[9], EGPO[10].

Some of these models are used in autonomous driving area on CARLA simulator, but some are not. Thus, we want to first move some of these models to the Metadrive[1] simulator.

We also so see a combination of policies that generates a better result such as Rainbow DQN. We will also propose a different way or a different combination of models to see if combination of policies leads to a better result.


### Platform
The platform we are going to work on is Metadrive[1]. It is an open-source driving simulator that is able to construct simple and complex scenarios.

## References
1. Li, Quanyi and Peng, Zhenghao and Xue, Zhenghai and Zhang, Qihang and Zhou, Bolei. "MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning." arXiv. 2021
2. Hester, Todd, et al. "Deep q-learning from demonstrations." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.
3. Hado van Hasselt, Arthur Guez, and David Silver. 2016. Deep reinforcement learning with double Q-Learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI'16). AAAI Press, 2094–2100.
4. Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Daniel Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, & David Silver (2017). Rainbow: Combining Improvements in Deep Reinforcement Learning. CoRR, abs/1710.02298.
5. Will Dabney, Mark Rowland, Marc G. Bellemare, & Rémi Munos (2017). Distributional Reinforcement Learning with Quantile Regression. CoRR, abs/1710.10044.
6. Sutton, R., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy Gradient Methods for Reinforcement Learning with Function Approximation. In Advances in Neural Information Processing Systems. MIT Press.
7. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, & Oleg Klimov (2017). Proximal Policy Optimization Algorithms. CoRR, abs/1707.06347.
8. Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., & Wierstra, D.. (2015). Continuous control with deep reinforcement learning.
9. Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., & Levine, S.. (2018). Soft Actor-Critic Algorithms and Applications.
10. Peng, Z, Li, Q & Liu, C & Zhou, B. "Safe Driving via Expert Guided Policy Optimization." 5th Annual Conference on Robot Learning. 2021.

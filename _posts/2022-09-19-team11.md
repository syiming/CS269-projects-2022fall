---
layout: post
comments: true
title: Move and Combine State-of-the-art Models in MetaDrive
author: Siqi Liu, Yiming Shi (Team 11)
date: 2022-10-12
---

> Many State-of-the-art results for reinforcement learning in autonomous driving are demonstrated over CARLA, VISTA. We aim to reproduce these results in Metadrive simulator, and compare them with Metadrive models. We will also investigate the combinations of different models in prediction stage.
<!-- (prev proposal)> Ususally driving involves multiple intelligent agents, intelligent machine or human. With more information cooperated, agents may performs differently to achieve their goals. In this projects, we want to research about Reinforcement Learning (RL) based method such that all the agents collaborate with each other to achieve the best goal in total. -->
<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction 
<!--Recently, Reinforcement learning has been wildly used in autonomous driving. Since the driving task has high complexity and hard to construct a safe model or policy, imitation learning by behavior cloning becomes popular as it allows the agent to be trained to learn from a human expert and thus quickly learn to plan the action based on the complex environment and take actions. However, imitation learning is also suffered from issues such as failing unseen complex scenarios. In this project, we are aiming on reproducing a safe driving model based on imitation learning, analyzing failing cases for imitation learning and exploring ways such as adding loss besides imitation loss to discourage certain undesired behavior.-->

<!-- As 5G techniques become more mature, vehicle-to-everything (V2X) becomes possible. This allows vehicles to communicate with any entity that may affect, or may be affected by, the vehicle. A vehicle may use this information to act differently to achieve the final goal.  -->

Recently, Reinforcement learning has been wildly used in autonomous driving and the driving task has high complexity and hard to construct a safe model or policy. Nowadays many different methods are developed and used in reinforcement learning, such as Deep Q-Network (Doubling DQN, Dueling DQN, Rainbow DQN), Policy Gradient, Natrual Policy Gradient, Proximal Policy Optimization, Advantage Actor-Critic, to name a few. A lot of these models are demonstrated on CARLA, which is a very popular simulator in autonomous driving. In this project, we are aiming at applying different models under a much light weighted and easy used simulator - Metadrive[1] and compare the result of these models.


## Proposed Work
The main goal of this project is to apply the following models using Metadrive and compare the result of these models. We also trying to reproduce some state-of-art models on Metadrive.
1. DQN
2. Double DQN
3. Rainbow DQN
4. Quantile Regressing DQN
5. Policy Gradient
6. Proximsl Policy Optimization
7. Deep Deterministic Policy Gradient
8. Soft Actor-Critic
9. EGPO

More models are going to be added.

In this project, we are also going compare different models about their advantange or disadvantage of each models (training time, performance, etc). We will also construct different senarios for different model to solve, we will compare the success rate, training time on these models under different senarios.

We also want to propose a way to combine different models in prediction stage. (eg. Majority vote for next action. If the action contains a continuous value, we could do an weighted sum.)

##  Reproducing SOTA Models in Metadrive

During this time, we spend some time investigating and understading different models, such as DQN, Double DQN, Rainbow DQN, Quantile Regressing DQN, Policy Gradient, Proximsl Policy Optimization, Deep Deterministic Policy Gradient, Soft Actor-Critic, EGPO.

Currently, we have installed and test several simple models on Metadrive, eg. DQN, Double DQN, PPO, Soft Actor-Critic, and EGPO.
The result of the models seems to be consistent with the paper; however, due to the resource constraint, we are not able to run all the seeds.

We are also testing tuning of the Soft Actor-Critic and PPO model, including using priorized replay. 

![ppo]({{ '/assets/images/team11/ppo.jpeg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
![sac]({{ '/assets/images/team11/sac.jpeg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}


Currently, we are continuing trying to work on more models with metadrive, and hopefully we can reproduce and visualize the result soon.


## Expected Result
We expect different result using different models with different training time, but for a combined policy with multiple models combined, we expect it to have better performance. We will also compare the perfomance for different models under simple and complex senarios separately. We anticipate the agent will reach a good performance on simple senario very quickly and for different models the time to reach the state will be different.

We will also train this models under different senarios, we expect the models are tend to solve the easy senarios very quick and might not solve the complex senarios due to computation resource limitation. The training time will be different and the action might be different by different models. We will try to analyze the differences for different models.

<!-- We will first try to reproduce the result with imitation learning, we are expecting the agent are able to dring under simple environment without crashing. We alse expect that the result with imitation learning plus other reinforcement learning, we will reach much better result on complex senarios. 
Concretely, we will allow our agent to learn from the human driving example, then use reinforcement learning to broaden the model's training to create a safer model. 

We will first reproduce the imitation learning in CheuffeurNet. Then, we will use a reinforcement technique similar to Deep q-learning from demonstrations. 
In the process, we may face obstacles in combining the model's approaches and training curves, and a solution may be to perform our training by stages, by first using CheuffeurNet's training, then DqFD's. 
Therefore, we expect to deliver a method of training autonomous driving with a combination of imitation learning and reinforcement learning to the readers.  -->

## Specialization Ensemble

In addition to the ensemble of multiple models, we also experimented with ensemble of models trained with specialization. 

### Environments

In this experiments, we uses environments configured from `MetaDrive-Tut-100Env-v0` above. 
To create specialized environments, we create environments with roads with `2, 3, 4` lanes, and the traffic density between `0.005, 0.01, 0.02` cars per 10 meters. 
This gives us a total of 9 specialized environments, named with format `MetaDrive-XHard-{}Lane-{}Traffic-v0`. 
The general environment, created from by sampling each of the 9 specialized environments randomly, is named `MetaDrive-XHard-v0`.

TODO Env plots

### Specialized models

We trained a specialized model for each of the specialized environment above. 
They are PPO models, trained with 4,000,000 time step with the same hyperparameters as the experiment before. 

### Generalized models

Similar to above, we have the following attempted generalized model:

1. The PPO model, a model trained with the same architecture as the specialized model, to use as a control. 
2. The average Generalized ensemble, a model created by using 9 PPO models on the generalized environment. 
3. The PPO-weighted Generalized ensemble model, a model that uses PPO to select weight for the weighted average from  9 PPO generalized models. 
4. The average Specialized ensemble, a model created by using equal-weighted average from specialized models. 
5. The PPO-weighted Specialized ensemble model, a model that uses PPO to select weight for the weighted average from specialized models. 

We have considered increasing the size of the single PPO model, but 1) we do not know what exact architecture would make a fair comparison, and 2) we are constrained by our computation resources. Also, we tried to give the single PPO model an extra 1,000,000 steps of training, to be consistent with the training used by the weight-selection PPO, but the training did not give better result. 

We did not perform hyper-parameter tuning; all the parameters are the same as the model used in assignment 3. However, we might select best model based the training rewards. 

We have also added a softmax layer after the weight-selection model to ensure each output weight is positive and sum as 1, yet it does not produce better results. 

### Results

#### Specialized models

The results are shown in the following table:

|Models| Training Reward | Training Success Rate |Evaluation Reward|
|-------------------------------------|-------|----|---|
| MetaDrive-XHard-2Lane-50Traffic-v0  |327.821|0.9|319.938|
| MetaDrive-XHard-2Lane-100Traffic-v0 |297.117|0.71|238.255|
| MetaDrive-XHard-2Lane-200Traffic-v0 |265.671|0.57|252.963|
| MetaDrive-XHard-3Lane-50Traffic-v0  |305.523|0.69|289.463|
| MetaDrive-XHard-3Lane-100Traffic-v0 |303.550|0.74|312.993|
| MetaDrive-XHard-3Lane-200Traffic-v0 |262.572|0.57|234.864|
| MetaDrive-XHard-4Lane-50Traffic-v0  |313.710|0.61|306.772|
| MetaDrive-XHard-4Lane-100Traffic-v0 |266.616|0.33|239.655|
| MetaDrive-XHard-4Lane-200Traffic-v0 |255.887|0.4|217.981|

#### Generalized models

The results are shown in the following table. 
The single PPO are reported as a distribution from the 9 models, in format of `(min, mean, max)`

|Models| Training Reward | Training Success Rate |Evaluation Reward|
|-------------------------------------|-------|----|---|
|Single PPO|(171.324, 224.220, 281.097)|(0.11, 0.33, 0.53)|(166.909, 221.985, 259.975)|
|Average Specialized Ensemble|N/A    |N/A |174.157|
|PPO Specialized Ensemble    |284.916|0.58|273.120|
|Average Generalized Ensemble|N/A    |N/A |228.523|
|PPO Generalized Ensemble    |279.211|0.58|275.704|

And the correponding training Curves for Single PPO model:

![single-agent-generalized]({{ '/assets/images/team11/single-agent-xhard.jpeg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}

And the correponding training Curves for Ensemble PPO model:

![ensemble-specialized-generalized]({{ '/assets/images/team11/ensemble-specialized-xhard.jpeg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}

And the correponding training Curves for Generalized Ensemble PPO model:

![ensemble-generalized-generalized]({{ '/assets/images/team11/ensemble-generalized-xhard.jpeg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}

We can see that the average ensemble models does not perform better than the single PPO in general. 
The ensemble models performs much better than the other models in evaluation, and in between them, the specialized ensemble performs slightly better, probably because it is able to accommodate more edge cases. 

Another thing we can notice is that the ensemble models have a smaller gap between training and evaluation rewards, compared to the single models. For the best model in Single PPO, it can achieve 281.087 in training, but only at most 258.975 in evaluation, while the ensemble models have the a gap of less than 5. 

#### Visualization

We continue to use the visualization from the above experiment, but modified to use 4 Lanes and 0.2 cars per 10 meters. This is the cases where the specialized model has the lowest score. 

For the single agent(Using the highest evaluation reward model), the model achieves 171.969 reward. 

![single-agent-vis]({{ '/assets/images/team11/single-agent-vis.gif' | relative_url }})
{: style="width: 1000px; max-width: 100%;"}

For the average generalized model, the model achieves 482.650 reward. 

![average-generalized-vis]({{ '/assets/images/team11/average-generalized-vis.gif' | relative_url }})
{: style="width: 1000px; max-width: 100%;"}

For the PPO generalized ensemble, the model achieves 518.062 reward.

![ensemble-generalized-vis]({{ '/assets/images/team11/ensemble-generalized-vis.gif' | relative_url }})
{: style="width: 1000px; max-width: 100%;"}

For the average specialized model, the model achieves 429.753 reward. 

![average-specialized-vis]({{ '/assets/images/team11/average-specialized-vis.gif' | relative_url }})
{: style="width: 1000px; max-width: 100%;"}

For the PPO specialized ensemble, the model achieves 525.138 reward.

![ensemble-specialized-vis]({{ '/assets/images/team11/ensemble-specialized-vis.gif' | relative_url }})
{: style="width: 1000px; max-width: 100%;"}

In this, we can see that both ensemble model is able to arrive at the destination correctly. The average models almost reached the destinations, and the single agent have failed to move through.  

#### Trained Weight analysis

We can also visualize the weight selection by the weight-selection PPO model. 
Because the original weight output from the model is too jittery, we use a exponential moving average to smooth the weights. The alpha used in moving average is 1/8. 

For the scenario in the visualization case, we outputted the weight. 
![weight-vis-4-2]({{ '/assets/images/team11/weight-vis-4-2.gif' | relative_url }})
{: style="width: 1000px; max-width: 100%;"}

From this visualization, we do not see the weight pattern for recognizing this scenario as 4-lanes, or as a scenario of heavy traffic. 
We only see that the 3-lane mid-traffic model is used the most overall, probably as the default when there is no obstacles. 
However, there are some patterns that are not evident at certain key areas of the driving:
1. At the T-section turning, the right turn is started by the high weight of 2-lane models, pulling the car to the side line. 
2. When the car is sourrounded by traffic after the T-section or after the roundabout, we see the 3-lane medium or heavy-traffic models are given more weight. 

We have also visualize the 2-lane scenario, which are driven poorly by the car. 

For the scenario that reduces number of lane to 2, we see the car failed and have only 87.517 reward. 
![weight-vis-2-2]({{ '/assets/images/team11/weight-vis-2-2.gif' | relative_url }})
{: style="width: 1000px; max-width: 100%;"}

For the scenario with 2 lanes and 0.05 traffic, we see the car failed and have only 87.852 reward, and the failure trajectory is about the same as the 2-lane heavy traffic scenario. 
![weight-vis-2-05]({{ '/assets/images/team11/weight-vis-2-05.gif' | relative_url }})
{: style="width: 1000px; max-width: 100%;"}

For the above 2 scenarios, we can see the cars have hit the mid-line. Before the crash, we can see that the 4-lane models have a slightly higher weight than 2-lane models. It is probably the 3 or 4-lane model tries to avoid the exit and go back to the middle of the imaginary 3 or 4-lane road, which crashes the vehicle into the midline. The model did not seems to learn that the 2-lane model's input is better in this case. 

#### Conclusion

We do see that our ensemble model is able to perform better than the single agent model. 
However, this better performance is mostly because of the robustness created by the use of differently trained model.
We do not see an interpretable pattern of weight-selection model recognizing the lane-number and traffic amount; 
instead, it starts to optimize toward the easier cases (high lane number), where policy changes in those can increase reward faster than in those harder cases. 
In the future, we may consider
1. Increase the importance of difficult scenarios by multiplying the reward. 
2. Allow the scenario to continue after the car crashes, but give some penalty. This allows the model to see the rewards after the crash, and not limited by the difficult problem in the beginning. It is also implemented in the Safe driving environment in Metadrive. 
We also see some weight patterns for specifically avoiding certain obstacles. 

## Issues
Due to our device constraints, we are not able to train all the models until convergence for many models. Therefore, there may be room for improvements for our PPO models. We also do not have enought time for model tuning. 

## Related Work

Currently, many reinforcement learning models are proposed and achieve great performance. We investiage many of these DQN[2], Double DQN[3], Rainbow DQN[4], Quantile Regressing DQN[5], Policy Gradient[6], Proximsl Policy Optimization[7], Deep Deterministic Policy Gradient[8], Soft Actor-Critic[9], EGPO[10].

Some of these models are used in autonomous driving area on CARLA simulator, but some are not. Thus, we want to first move some of these models to the Metadrive[1] simulator.

We also so see a combination of policies that generates a better result such as Rainbow DQN. We will also propose a different way or a different combination of models to see if combination of policies leads to a better result.


### Platform
The platform we are going to work on is Metadrive[1]. It is an open-source driving simulator that is able to construct simple and complex scenarios.

## References
1. Li, Quanyi and Peng, Zhenghao and Xue, Zhenghai and Zhang, Qihang and Zhou, Bolei. "MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning." arXiv. 2021
2. Hester, Todd, et al. "Deep q-learning from demonstrations." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.
3. Hado van Hasselt, Arthur Guez, and David Silver. 2016. Deep reinforcement learning with double Q-Learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI'16). AAAI Press, 2094–2100.
4. Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Daniel Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, & David Silver (2017). Rainbow: Combining Improvements in Deep Reinforcement Learning. CoRR, abs/1710.02298.
5. Will Dabney, Mark Rowland, Marc G. Bellemare, & Rémi Munos (2017). Distributional Reinforcement Learning with Quantile Regression. CoRR, abs/1710.10044.
6. Sutton, R., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy Gradient Methods for Reinforcement Learning with Function Approximation. In Advances in Neural Information Processing Systems. MIT Press.
7. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, & Oleg Klimov (2017). Proximal Policy Optimization Algorithms. CoRR, abs/1707.06347.
8. Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., & Wierstra, D.. (2015). Continuous control with deep reinforcement learning.
9. Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., & Levine, S.. (2018). Soft Actor-Critic Algorithms and Applications.
10. Peng, Z, Li, Q & Liu, C & Zhou, B. "Safe Driving via Expert Guided Policy Optimization." 5th Annual Conference on Robot Learning. 2021.

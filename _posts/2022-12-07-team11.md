---
layout: post
comments: true
title: Ensemble RL - Apply Ensemble Methods on Reinforcement Models using MetaDrive
author: Siqi Liu, Yiming Shi (Team 11)
date: 2022-10-12
---

>  Ensemble method helps improve models' performance by combining multiple models instead of using a single model. These methods are wildly used in many Machine Learning tasks. However, there is not much implementation in the Reinforcement Learning area. In this project, we will apply ensemble methods in Reinforcement Learning to our autonomous driving task based on the MetaDrive platform. We trained Proximal Policy Optimization (PPO), Twin Delayed DDPG (TD3), Generative Adversarial Imitation Learning (GAIL), and Soft Actor-Critic (SAC) models as baseline models. We investigated different ensemble methods based on these models. The overall result for the model after the ensemble is slightly better than the ones without the ensemble, but in some cases, we gain much better results.
<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## 1 Introduction 
Recently, Reinforcement learning has been wildly used in autonomous driving and the driving task has high complexity, and is hard to construct a safe model or policy. Nowadays many different methods are developed and used in reinforcement learning, such as Proximal Policy Optimization (PPO)[1], Twin Delayed DDPG (TD3)[2], Generative Adversarial Imitation Learning (GAIL)[3], and Soft Actor-Critic (SAC)[4], to name a few. These models achieve pretty impressive results on autonomous driving tasks so far by themselves but currently no published effort tried to improve performance by combining these models. In this project, we investigate different ways of combining these methods and checked if there are improvements.

One popular method that could easily combine different models is the ensemble method. Ensemble methods[5] are techniques that aim at improving the accuracy of results in models by combining multiple models instead of using a single model. These are wildly used machine learning problems such as classification and improving the accuracy by taking a (weighted) vote of their predictions. There are also ensemble methods such as bagging and boosting. 

In this project, we investigated whether applying ensemble methods to reinforcement learning algorithms in autonomous driving tasks will improve rewards and performance. We investigated several different ways to ensemble different models. We trained and tested the following several ensemble methods: (1) Ensemble by taking a (weighted) vote of their predictions. (2) Ensemble by using a human-defined strategy of predictions. (3) Learning the weight of different models using reinforcement learning and ensemble by taking a (weighted) vote of their predictions. The overall result for the model after the ensemble is slightly better than the ones without the ensemble, but in some cases, we gain much better results.

The paper is organized as follows. Section 2 summarized all the related works in reinforcement learning, autonomous driving simulation platforms, and ensemble methods.  Section 3 showed the basedline models and their performance. Section 4 illustrated all our methods in detail and presented the experiment settings and experiment results. Section 5 demonstrated our conclusion. And we briefly talked about our future works in section 6.


## 2 Related Works
### 2.1 Platform

### 2.2 SOTA Reinforcement Learning Models

### 2.3 Ensemble Methods


## 3 Base Model Performance

| Models | Episodic Reward |
| ------ | :-------------: |
| PPO    |     194.027     |
| TD3    |     340.056     |
| GAIL   |     269.660     |
| SAC    |     273.224     |
{: style="margin-left: auto; margin-right: auto; max-width: 70%; text-align: center"}
*Table 1. Base Model Performance.*
{: style="text-align: center"}

## 4 Methodology and Experiments
In this section, we will investigate different ways of ensembling several pretrained models. We mainly tried the following three different ways to ensemble models. (1) Ensemble by taking a (weighted) vote of their predictions. (2) Ensemble by using a human-defined strategy of predictions. (3) Ensemble using reinforcement learning.

### 4.1 Naive Average with Weight
The first ensemble method we use is taking a weighted vote of baseline models' predicted actions. The predicted action after the ensemble is calculated using the following equation

$$
a=\sum_i w_i a_i
$$, subject to
$$\sum_i w_i = 1$$
{: style="text-align: center"}

where $$a_i$$ is the prediction action from the $$i$$-th model and $$w_i$$ is a hyperparamter representing the weight for each parameter. We also make sure the sum of all weights is equal to 1.

In this method, during the prediction phase, when we get an observation from the environment, we pass the observation to the baseline models and collect predicted actions from them. The final action we use is the weighted average of the collected actions.

![Avg]({{ '/assets/images/team11/avg-weight.png' | relative_url }})
{: style="margin-left: auto; margin-right: auto; max-width: 70%; text-align: center"}
*Fig 1. Ensemble by Naive Average with Weight.*
{: style="text-align: center"}
  
#### 4.1.1 Experiment Result and Visualization
We experiment with the methods with different combinations of baseline models trained on MetaDrive Environment and evaluate the model on 10 complicated environments using MetaDrive. The combinations and testing results we tested are shown in the table below.

| Models         | Weight | Episodic Reward |
| :------------- | :----: | :-------------: |
| PPO, GAIL      |  1:1   |     216.536     |
| PPO, TD3       |  1:1   |     298.235     |
| PPO, TD3, GAIL | 1:1:1  |     279.224     |
| TD3, PPO, SAC  | 1:1:1  |     324.545     |
| TD3, PPO, SAC  | 2:1:2  |     343.507     |
| TD3, PPO, SAC  | 2:2:1  |     322.389     |
| TD3, PPO, SAC  | 1:2:2  |     315.341     |
| TD3, PPO, SAC  | 3:1:1  |     370.225     |
| TD3, PPO, GAIL | 1:1:8  |     268.913     |
| TD3, PPO, GAIL | 8:1:1  |     231.019     |
{: style="margin-left: auto; margin-right: auto; max-width: 70%; text-align: center"}
*Table 2. Experiment Result for Ensemble by Naive Average with Weight.*
{: style="text-align: center"}

Based on table 2, compare to a single model performance (baseline performance) shown in table one, we found that, in most cases, the ensembled reward is better than the worst baseline it used and worst than the best baseline models it used. In some cases, the reward of the ensembled model is better than all the models it used. 

We also visualized the prediction with the best ensembled model (TD3, PPO, SAC, 3:1:1) on one of our selected environments as below.

![Naive-vis]({{ '/assets/images/team11/average-309.512.gif' | relative_url }})
{: style="margin-left: auto; margin-right: auto; max-width: 80%; text-align: center"}
*Fig 2. Ensemble with Human-Defined Strategy.*
{: style="text-align: center"}

### 4.2 Ensemble with Human-Defined Strategy
The first method, Naive Average with Weight, couldn't increase the performance significantly. We believe one possible problem for it might be averaging might leads to actions doesn't make sense. Consider the following scenario (Fig. 3). Suppose we want to pass over the gray car in front of our green agent. Model 1 suggests we turn left and accelerate.
Model 2 suggests we turn right and accelerate. If we average these two actions the ensembled action will be going straight and accelerating, in which case we will just hit the grey car in front of us.

![Problematic_Senario]({{ '/assets/images/team11/senario.png' | relative_url }})
{: style="margin-left: auto; margin-right: auto; max-width: 50%; text-align: center"}
*Fig 3. Problematic Senario using Ensemble by Naive Average with Weight.*
{: style="text-align: center"}

To avoid this situation from happening, we construct the following strategy (Fig. 4). Using the strategy, during the prediction phase, when we get an observation from the environment, we pass the observation to the baseline models and collect predicted actions from them. We then do a majority vote on the directions, then the final ensembled action should be the average of the actions with the majority actions.

![Strategy]({{ '/assets/images/team11/avg-strategy.png' | relative_url }})
{: style="margin-left: auto; margin-right: auto; max-width: 80%; text-align: center"}
*Fig 4. Ensemble with Human-Defined Strategy.*
{: style="text-align: center"}

The final action is calculated by

$$
\textit{major_direction} = \textit{majority} (sign(a_i[0]))
$$

$$
a=avg(\mathbb{1}(sign(a_i[0]) = \textit{major_direction})a_i)
$$
{: style="text-align: center"}

Take Fig. 4 as an example, suppose we have three models, PPO, SAC, and TD3 model. PPO, SAC predicts turning left. TD3 Predicts turning right. Then the majority directions will be turning left. Thus the final action we take will be the average of action of PPO and SAC.

#### 4.2.1 Experiment Result and Visualization
We experiment with the methods with 2 combinations of baseline models trained on MetaDrive Environment and evaluate the model on 10 complicated environments using MetaDrive. The combinations and testing results we tested are shown in the table below.

| Models         | Episodic Reward |
| -------------- | :-------------: |
| TD3, PPO, SAC  |     363.580     |
| PPO, TD3, GAIL |     268.913     |
{: style="margin-left: auto; margin-right: auto; max-width: 70%; text-align: center"}
*Table 3. Experiment Result for Ensemble with Human-Defined Strategy.*
{: style="text-align: center"}

Based on Table 3, compared with the baseline model performance in Table 1, and naive average with weight method performance in Table 2. We found that the performance is pretty close to the performance of naive average with weight method. For TD3, PPO, SAC combination, we have reward 363.580 on our test environments, which is better than 324.545 using naive average with weight method. While for PPO, TD3, GAIL combination we have reward 268.913, which is slightly worse than 279.224 using naive average with weight method.

We also visualized the prediction with the best ensembled model (TD3, PPO, SAC) on one of our selected environments as below.

![Naive-vis]({{ '/assets/images/team11/VAV-221,799.gif' | relative_url }})
{: style="margin-left: auto; margin-right: auto; max-width: 80%; text-align: center"}
*Fig 2. Ensemble with Human-Defined Strategy.*
{: style="text-align: center"}

### 4.3 Learned Ensemble using Reinforcement Learning
In section 4.1 and 4.2, we introduced two methods that 
#### 4.3.1 Ensemble Different Models
##### Experiment Result and Visualization

#### 4.3.2 Ensemble Different PPO Models Trained on Different Senarios
##### Experiment Result and Visualization

#### 4.3.3 Ensemble Different PPO Models Trained on Different Traffic Situations
##### Experiment Result and Visualization

## 5 Conclusion

## 6 Future Works

## 7 References
1. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, & Oleg Klimov (2017). Proximal Policy Optimization Algorithms. CoRR, abs/1707.06347.
2. Fujimoto, S., Hoof, H., & Meger, D.. (2018). Addressing Function Approximation Error in Actor-Critic Methods.
3. Ho, J., & Ermon, S.. (2016). Generative Adversarial Imitation Learning.
4. Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S.. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.
5. Dietterich, T.G. (2000). Ensemble Methods in Machine Learning. In: Multiple Classifier Systems. MCS 2000. Lecture Notes in Computer Science, vol 1857. Springer, Berlin, Heidelberg. https://doi.org/10.1007/3-540-45014-9_1


---
layout: post
comments: true
title: Ensemble RL - Apply Ensemble Methods on Reinforcement Models using MetaDrive
author: Siqi Liu, Yiming Shi (Team 11)
date: 2022-10-12
---

>  Ensemble method helps improve models' performance by combining multiple models instead of using a single model. These methods are wildly used in many Machine Learning tasks. However, there is not much implementation in the Reinforcement Learning area. In this project, we will apply ensemble methods in Reinforcement Learning to our autonomous driving task based on the MetaDrive platform. We trained Proximal Policy Optimization (PPO), Twin Delayed DDPG (TD3), Generative Adversarial Imitation Learning (GAIL), and Soft Actor-Critic (SAC) models as baseline models. We investigated different ensemble methods based on these models. The overall result for the model after the ensemble is slightly better than the ones without the ensemble, but in some cases, we gain much better results.
<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction 
Recently, Reinforcement learning has been wildly used in autonomous driving and the driving task has high complexity, and is hard to construct a safe model or policy. Nowadays many different methods are developed and used in reinforcement learning, such as Proximal Policy Optimization (PPO)[1], Twin Delayed DDPG (TD3)[2], Generative Adversarial Imitation Learning (GAIL)[3], and Soft Actor-Critic (SAC)[4], to name a few. These models achieve pretty impressive results on autonomous driving tasks so far by themselves but currently no published effort tried to improve performance by combining these models. In this project, we investigate different ways of combining these methods and checked if there are improvements.

One popular method that could easily combine different models is the ensemble method. Ensemble methods[5] are techniques that aim at improving the accuracy of results in models by combining multiple models instead of using a single model. These are wildly used machine learning problems such as classification and improving the accuracy by taking a (weighted) vote of their predictions. There are also ensemble methods such as bagging and boosting. 

In this project, we investigated whether applying ensemble methods to reinforcement learning algorithms in autonomous driving tasks will improve rewards and performance. We investigated several different ways to ensemble different models. We trained and tested the following several ensemble methods: (1) Ensemble by taking a (weighted) vote of their predictions. (2) Ensemble by using a human-defined strategy of predictions. (3) Learning the weight of different models using reinforcement learning and ensemble by taking a (weighted) vote of their predictions. The overall result for the model after the ensemble is slightly better than the ones without the ensemble, but in some cases, we gain much better results.

The paper is organized as follows. Section 2 summarized all the related works in reinforcement learning, autonomous driving simulation platforms, and ensemble methods.  Section 3 illustrated all our methods in detail and presented the experiment settings and experiment results. Section 4 demonstrated our conclusion. And we briefly talked about our future works in section 5.


## Related Works
### Platform

### SOTA Reinforcement Learning Models

### Ensemble Methods


## Methodology and Experiments
In this section, we will investigate different ways of ensembling several pretrained models. We mainly tried the following three different ways to ensemble models. (1) Ensemble by taking a (weighted) vote of their predictions. (2) Ensemble by using a human-defined strategy of predictions. (3) Ensemble using reinforcement learning.

### Naive Average with Weight
#### Experiment Result and Visualization

### Ensemble with Human-Defined Strategy
#### Experiment Result and Visualization

### Learned Ensemble using Reinforcement Learning
#### Ensemble Different Models
##### Experiment Result and Visualization

#### Ensemble Different PPO Models Trained on Different Senarios
##### Experiment Result and Visualization

#### Ensemble Different PPO Models Trained on Different Traffic Situations
##### Experiment Result and Visualization

## Conclusion

## Future Works

## References
1. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, & Oleg Klimov (2017). Proximal Policy Optimization Algorithms. CoRR, abs/1707.06347.
2. Fujimoto, S., Hoof, H., & Meger, D.. (2018). Addressing Function Approximation Error in Actor-Critic Methods.
3. Ho, J., & Ermon, S.. (2016). Generative Adversarial Imitation Learning.
4. Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S.. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.
5. Dietterich, T.G. (2000). Ensemble Methods in Machine Learning. In: Multiple Classifier Systems. MCS 2000. Lecture Notes in Computer Science, vol 1857. Springer, Berlin, Heidelberg. https://doi.org/10.1007/3-540-45014-9_1

